{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID_FakeNews_DataPreparation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFQ3fiiiYgqH",
        "outputId": "bea6e990-53d4-4d51-d46a-137c5b516885"
      },
      "source": [
        "!pip install textstat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (0.7.1)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat) (0.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56P_0PHrRWeq"
      },
      "source": [
        "import re as re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import textstat\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import hashlib\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GP7CRKWSTJD"
      },
      "source": [
        "# path configuration\n",
        "RAW_DATA_URL = \"https://github.com/INTERTECHNICA-BUSINESS-SOLUTIONS-SRL/COVID-Fake-News-Analysis/raw/main/data/original/fake_new_dataset.xlsx\";\n",
        "STOP_WORDS_DATA_URL = \"https://github.com/INTERTECHNICA-BUSINESS-SOLUTIONS-SRL/COVID-Fake-News-Analysis/raw/main/data/support/stop_words_tokens.csv\"\n",
        "\n",
        "PROCESSED_DATA_FILE_NAME = './COVIDFakeNewsProcessedData.csv'\n",
        "ARCHIVED_DATA_FILE_PATH = \"./data/processed\"\n",
        "ARCHIVED_DATA_FILE_NAME = \"COVIDFakeNewsProcessedData.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2Asyuh14YkV"
      },
      "source": [
        "# read basic data\n",
        "raw_data = pd.read_excel(RAW_DATA_URL, index_col=0);\n",
        "\n",
        "# read stop words data\n",
        "stop_words_raw_data = pd.read_csv(STOP_WORDS_DATA_URL, index_col=None ,header=0);\n",
        "stop_words_data = stop_words_raw_data[\"tokens\"].values;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBKdw9MjuRnA"
      },
      "source": [
        "# creates numeric label values: 1 - false news, 0 - veridic news\n",
        "# creates numeric subcategory values: 2 - false news, 1 - partially false news, 0 - veridic news \n",
        "class LabelPreparationTransformer (BaseEstimator, TransformerMixin) :\n",
        "  def binary_target_mapper(self, x): \n",
        "    if x == 1 : return 0;\n",
        "    if x == 0 : return 1;\n",
        "    return None;\n",
        "\n",
        "  def multi_target_mapper(self, x): \n",
        "    if x == 'false news' : return 2;\n",
        "    if x == 'partially false' : return 1;\n",
        "    if x == 'true' : return 0;\n",
        "    return None;\n",
        "\n",
        "  def fit(self, X) :\n",
        "      return self;\n",
        "\n",
        "  def transform(self, X, Y = None) :\n",
        "    X[\"binary_target\"] = X[\"label\"].apply(lambda x: self.binary_target_mapper(x));\n",
        "    X[\"multi_target\"] = X[\"subcategory\"].apply(lambda x: self.multi_target_mapper(x));\n",
        "\n",
        "    X = X.drop([\"label\", \"subcategory\"], axis = 1);\n",
        "\n",
        "    return X;    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI578nlfu7aT"
      },
      "source": [
        "# concatenates title and text for generating the content\n",
        "# minimal cleanup for empty values \n",
        "class BasicPreparationTransformer (BaseEstimator, TransformerMixin) :\n",
        "\n",
        "  def fit(self, X) :\n",
        "      return self;\n",
        "\n",
        "  def transform(self, X, Y = None) :\n",
        "    X[\"title\"] = X[\"title\"].replace(np.NaN, \"\");\n",
        "    X[\"text\"] = X[\"text\"].replace(np.NaN, \"\");\n",
        "\n",
        "    X[\"content\"] = X[\"title\"] + \" \" + X[\"text\"];\n",
        "\n",
        "    X[\"content\"] = X[\"content\"].apply(lambda x : x.strip());\n",
        "\n",
        "    X = X.drop([\"title\", \"text\"], axis = 1);\n",
        "\n",
        "    return X;    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6c5ojOQLZwW"
      },
      "source": [
        "# removes duplicate news from the dataset\n",
        "class DuplicateRemoverTransformer(BaseEstimator, TransformerMixin) :\n",
        "  def __init__(self) :\n",
        "      super().__init__();\n",
        "      self._lowercase_letters_only = r'[^a-z]'; \n",
        "      \n",
        "  def hash(self, x) :\n",
        "      hash_source = x.lower();\n",
        "      hash_source = re.sub(self._lowercase_letters_only, '', hash_source)\n",
        "      hash_value = hashlib.sha1(hash_source.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "      return hash_value;  \n",
        "\n",
        "  def fit(self, X) :\n",
        "      return self;\n",
        "\n",
        "  def transform(self, X, Y = None) :\n",
        "    X[\"hash\"] = X[\"content\"].apply(lambda x : self.hash(x));\n",
        "    X = X.drop_duplicates(subset='hash', keep=\"first\");\n",
        "    X = X.drop([\"hash\"], axis = 1);\n",
        "\n",
        "    return X;      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iezqUM4pmtBa"
      },
      "source": [
        "# tokenizes the text\n",
        "# removes stop words, numbers and other superfluous information\n",
        "class TokenizationTransformer (BaseEstimator, TransformerMixin) :\n",
        "  def __init__(self, nlp_support, custom_stop_words_tokens) :\n",
        "    super().__init__()\n",
        "    self._nlp_support = nlp_support;\n",
        "    self._covid_19_regexp = r\"-19\";    \n",
        "    self._covid_regexp = r\"covid\";\n",
        "    self._number_regexp = r\"[0-9]+\";\n",
        "    self._stemmer = SnowballStemmer(\"english\");\n",
        "    self._custom_stop_words_tokens = custom_stop_words_tokens;\n",
        "\n",
        "\n",
        "  def is_valid(self, token) :\n",
        "    # remove tokens based on their syntatic information \n",
        "    is_invalid = token.is_stop or \\\n",
        "      token.is_punct or \\\n",
        "      token.is_left_punct or \\\n",
        "      token.is_right_punct or \\\n",
        "      token.is_space or \\\n",
        "      token.is_bracket or \\\n",
        "      token.is_quote or \\\n",
        "      token.is_currency or \\\n",
        "      token.like_url or \\\n",
        "      token.like_num or \\\n",
        "      token.like_email;\n",
        "\n",
        "    return not is_invalid;\n",
        "\n",
        "  def prepare(self, text) :\n",
        "    prepared_text = text.strip();  \n",
        "    prepared_text = prepared_text.lower();\n",
        "    prepared_text = re.sub(self._covid_19_regexp, \"\", prepared_text);\n",
        "    prepared_text = re.sub(self._number_regexp, \"\", prepared_text);\n",
        "    prepared_text = self._stemmer.stem(prepared_text);\n",
        "    \n",
        "    if (len(prepared_text) == 0) :\n",
        "      return None;\n",
        "\n",
        "    if (prepared_text in self._custom_stop_words_tokens):\n",
        "      return None;  \n",
        "\n",
        "    return prepared_text;\n",
        "\n",
        "  def fit(self, X) :\n",
        "      return self;\n",
        "\n",
        "  def transform(self, X, Y = None) :\n",
        "    X[\"tokens_text_joined\"] = np.repeat(None, X.shape[0]);\n",
        "    X[\"tokens_text_lemma_joined\"] = np.repeat(None, X.shape[0]);\n",
        "    X[\"tokens_pos_joined\"] = np.repeat(None, X.shape[0]);\n",
        "    X[\"tokens_text_processed_joined\"] = np.repeat(None, X.shape[0]);\n",
        "                                         \n",
        "    for i in range(0, X.shape[0]) :\n",
        "      tokens = self._nlp_support(X.iloc[i][\"content\"]);\n",
        "      tokens_text = [];\n",
        "      tokens_text_lemma = [];\n",
        "      tokens_text_pos = [];\n",
        "      tokens_text_processed = [];\n",
        "\n",
        "      for token in tokens:\n",
        "        if (self.is_valid(token)) :\n",
        "          prepared_text = self.prepare(token.text); \n",
        "          if (prepared_text == None) :\n",
        "            continue;\n",
        "          tokens_text = np.append(tokens_text, token.text);   \n",
        "          tokens_text_lemma = np.append(tokens_text_lemma, token.lemma_);\n",
        "          tokens_text_pos = np.append(tokens_text_pos, token.pos_);\n",
        "          tokens_text_processed = np.append(tokens_text_processed, prepared_text);\n",
        "\n",
        "      X.iloc[i, X.columns.get_loc(\"tokens_text_joined\")] = \" \".join(tokens_text);\n",
        "      X.iloc[i, X.columns.get_loc(\"tokens_text_lemma_joined\")] = \" \".join(tokens_text_lemma);\n",
        "      X.iloc[i, X.columns.get_loc(\"tokens_pos_joined\")] = \" \".join(tokens_text_pos);\n",
        "      X.iloc[i, X.columns.get_loc(\"tokens_text_processed_joined\")] = \" \".join(tokens_text_processed);\n",
        "\n",
        "    return X;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxR_ODRTdTVY"
      },
      "source": [
        "# determine the sentence count for news records  \n",
        "class SentencesCountTransformer(BaseEstimator, TransformerMixin) :\n",
        "  def __init__(self, nlp_support) :\n",
        "    super().__init__()\n",
        "    self._nlp_support = nlp_support;\n",
        "\n",
        "  def get_sentences_count(self, x) :\n",
        "    doc = self._nlp_support(x);\n",
        "    return len(list(doc.sents));\n",
        "\n",
        "  def fit(self, X) :\n",
        "    return self;  \n",
        "\n",
        "  def transform(self, X, Y = None) :\n",
        "    X[\"sentences_count\"] = X[\"content\"].apply(lambda x : self.get_sentences_count(x));\n",
        "    \n",
        "    return X;\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UfIHYqEi5qt"
      },
      "source": [
        "# determine the sentiment for news records  \n",
        "class SentimentTransformer(BaseEstimator, TransformerMixin) :\n",
        "  def fit(self, X) :\n",
        "    return self;  \n",
        "\n",
        "  def get_sentiment_subjectivity(self, x):\n",
        "    blob = TextBlob(x);\n",
        "    return blob.sentiment.subjectivity;\n",
        "\n",
        "  def get_sentiment_polarity(self, x):\n",
        "    blob = TextBlob(x);\n",
        "    return blob.sentiment.polarity;\n",
        "\n",
        "  def transform(self, X, Y = None) :\n",
        "    X[\"sentiment_polarity\"] = X[\"content\"].apply(lambda x : self.get_sentiment_polarity(x));\n",
        "    X[\"sentiment_subjectivity\"] = X[\"content\"].apply(lambda x : self.get_sentiment_subjectivity(x));\n",
        "    \n",
        "    return X;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkX-dL4KrM_p"
      },
      "source": [
        "# determine the readability score (Fleschâ€“Kincaid) for news records  \n",
        "class ReadabilityScoreTransformer(BaseEstimator, TransformerMixin) :\n",
        "  def fit(self, X) :\n",
        "    return self;  \n",
        "\n",
        "  def get_readability_score(self, x):\n",
        "      readability_score = textstat.flesch_reading_ease(x);\n",
        "      return readability_score;\n",
        "\n",
        "  def transform(self, X, Y = None) :\n",
        "    X[\"readability_score\"] = X[\"content\"].apply(lambda x : self.get_readability_score(x));\n",
        "    \n",
        "    return X;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Eh1Wuuv5l6"
      },
      "source": [
        "# create the data processing pipeline\n",
        "transformation_pipeline = Pipeline([\n",
        "    (\"LabelPreparationTransformer\", LabelPreparationTransformer()),\n",
        "    (\"BasicPreparationTransformer\", BasicPreparationTransformer()),\n",
        "    (\"DuplicateRemoverTransformer\", DuplicateRemoverTransformer()),\n",
        "    (\"TokenizationTransformer\", TokenizationTransformer(nlp, stop_words_data)),\n",
        "    (\"SentencesCountTransformer\", SentencesCountTransformer(nlp)),\n",
        "    (\"SentimentTransformer\", SentimentTransformer()),\n",
        "    (\"ReadabilityScoreTransformer\", ReadabilityScoreTransformer())\n",
        "]);\n",
        "\n",
        "# process the pipeline\n",
        "processed_data = transformation_pipeline.fit_transform(raw_data);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aNLeHXyY9m_",
        "outputId": "e474ef33-bad5-462a-c7c4-0a3cca944445"
      },
      "source": [
        "# data processing report\n",
        "print(\"Processed records: \\t {}\".format(processed_data.shape[0]));\n",
        "print(\"Removed records: \\t {}\".format(raw_data.shape[0] - processed_data.shape[0]));"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed records: \t 3002\n",
            "Removed records: \t 117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfmObG4pdPQ9"
      },
      "source": [
        "# write the processed data\n",
        "output_path = Path(ARCHIVED_DATA_FILE_PATH);\n",
        "output_path.mkdir(parents=True, exist_ok=True);\n",
        "\n",
        "compression_options = dict(method = 'zip', archive_name = PROCESSED_DATA_FILE_NAME); \n",
        "processed_data.to_csv(\n",
        "    ARCHIVED_DATA_FILE_PATH + \"/\" + ARCHIVED_DATA_FILE_NAME, \n",
        "    header = True, \n",
        "    index = False, \n",
        "    compression = compression_options\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}